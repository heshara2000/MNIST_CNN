{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mR0OfaXTbUOq"
      },
      "outputs": [],
      "source": [
        "# prompt: import torch\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data transforms  ---> first tensors,then Image normalisation\n",
        "#these values are used to normalized the image tensors\n",
        "mean = 0.1307\n",
        "std = 0.3081\n",
        "\n",
        "#transformer for train and test data\n",
        "transform_original = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "\n",
        "#transformer for new image (inference)\n",
        "transform_inference = transforms.Compose([\n",
        "    transforms.Resize((28,28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "#load train n test data\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train = True,\n",
        "    transform = transform_original,\n",
        "    download =True\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train = True,\n",
        "    transform = transform_original,\n",
        "    download =True\n",
        ")"
      ],
      "metadata": {
        "id": "WnuVcxr8cuk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860c3e1e-3f20-4033-d7db-7052659504e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 143376387.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 101453679.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 47003074.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4406784.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 100\n",
        "epochs= 10\n",
        "lr=0.001\n",
        "\n",
        "#creating training ad test minibatches\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "print(f\"No of images in the train dataset: {len(train_dataset)}\")\n",
        "print(f\"No of images in the test dataset: {len(test_dataset)}\")\n",
        "print(f\"No of batches in the train dataset: {len(train_loader)}\")\n",
        "print(f\"No of batches in the test dataset: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C8hXCGMEcER",
        "outputId": "13173dbd-05fc-435b-c547-5adc3c0e0a71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of images in the train dataset: 60000\n",
            "No of images in the test dataset: 60000\n",
            "No of batches in the train dataset: 600\n",
            "No of batches in the test dataset: 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the networ architecture\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # first conv layer\n",
        "    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    #batch normalization----> normalizing output feature maps on 1st convolution layer\n",
        "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "\n",
        "    #ReLU\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    #max poooinmg\n",
        "    self.maxpoool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    #after the maxpool layer the size become 14 x 14\n",
        "\n",
        "    #second conv layer\n",
        "    self.cnn2= nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpoool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    #afterthe 2 nd convolution + Max pooling the size become 7 x 7\n",
        "\n",
        "\n",
        "    ##introducing fully connected layer ===> 7 x7 x 32\n",
        "\n",
        "    self.fc1 = nn.Linear(\n",
        "        in_features= 1568,\n",
        "        out_features=600  #u can decide it\n",
        "\n",
        "    )\n",
        "\n",
        "    ##adding dropout layer\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear (in_features=600, out_features=10)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_image):\n",
        "    # Conv Layer 01\n",
        "    out = self.cnn1(input_image)\n",
        "    out = self.batchnorm1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpoool1(out)\n",
        "\n",
        "\n",
        "    # Conv Layer 02\n",
        "    out = self.cnn2(out)\n",
        "    out = self.batchnorm2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpoool2(out)\n",
        "\n",
        "    # Flatterning the feature maps\n",
        "    out = out.view(-1, 1568)\n",
        "\n",
        "    # Fully connected layers\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IeKDO7jlzTgV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the objects\n",
        "\n",
        "cnn = CNN()\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "if CUDA:\n",
        "  cnn = cnn.cuda()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr)"
      ],
      "metadata": {
        "id": "3vq8PkfkHKeC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(epochs):\n",
        "  # Single epoch\n",
        "  iteration = 0\n",
        "  correct_predictions = 0\n",
        "  iteration_loss = 0\n",
        "\n",
        "  # Putting the model into train mode\n",
        "  cnn.train()\n",
        "\n",
        "  print(f\"\\niteration {epoch+1} is starting...\\n\")\n",
        "\n",
        "  #Training\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    # A single mini batch (100 images + labels)\n",
        "    inputs = Variable(inputs)\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    # deploy images and labels into gpu\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = cnn(inputs) # outputs for 100 images\n",
        "\n",
        "    loss_val = loss(output, labels) # Calculate the loss for 100 images\n",
        "\n",
        "    iteration_loss = iteration_loss + loss_val.item()\n",
        "\n",
        "    # back propagation\n",
        "    loss_val.backward()\n",
        "\n",
        "    # weight update\n",
        "    optimizer.step()\n",
        "\n",
        "    # Getting the number with highest probability\n",
        "    _, predicted_numbers = torch.max(output, 1)\n",
        "\n",
        "    correct_predictions = correct_predictions + (predicted_numbers == labels.data).sum()\n",
        "\n",
        "    iteration = iteration +1\n",
        "\n",
        "    train_accuracy.append((correct_predictions / len(train_dataset) * 100))\n",
        "\n",
        "  print(f\"Train Accuracy for Epoch{epoch+1}: {(correct_predictions / len(train_dataset) * 100)}\")\n",
        "\n",
        "  #Testing\n",
        "  test_loss = 0\n",
        "  test_correct = 0\n",
        "  iteration = 0\n",
        "\n",
        "  #Put the model into evaluation mode (disable the backpropagation features)\n",
        "  cnn.eval()\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(test_loader):\n",
        "    # A single mini batch (100 images + labels)\n",
        "    inputs = Variable(inputs)\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    # deploy images and labels into gpu\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    output = cnn(inputs) # outputs for 100 images\n",
        "\n",
        "    loss_val = loss(output, labels) # Calculate the loss for 100 images\n",
        "\n",
        "    test_loss = test_loss + loss_val.item()\n",
        "\n",
        "    # Getting the number with highest probability\n",
        "    _, predicted_numbers = torch.max(output, 1)\n",
        "\n",
        "    test_correct = test_correct + (predicted_numbers == labels.data).sum()\n",
        "\n",
        "    iteration = iteration +1\n",
        "\n",
        "    test_accuracy.append((test_correct / len(test_dataset) * 100))\n",
        "\n",
        "  print(f\"Test Accuracy for Epoch {epoch+1}: {(test_correct / len(test_dataset) * 100)}\")\n"
      ],
      "metadata": {
        "id": "VioR_y76I6Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b75e94-bd33-4705-953c-d172a8c068a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "iteration 1 is starting...\n",
            "\n",
            "Train Accuracy for Epoch1: 60.95833206176758\n",
            "Test Accuracy for Epoch 1: 85.98500061035156\n",
            "\n",
            "iteration 2 is starting...\n",
            "\n",
            "Train Accuracy for Epoch2: 83.72000122070312\n",
            "Test Accuracy for Epoch 2: 90.21833801269531\n",
            "\n",
            "iteration 3 is starting...\n",
            "\n",
            "Train Accuracy for Epoch3: 88.16166687011719\n",
            "Test Accuracy for Epoch 3: 92.15166473388672\n",
            "\n",
            "iteration 4 is starting...\n",
            "\n",
            "Train Accuracy for Epoch4: 90.3183364868164\n",
            "Test Accuracy for Epoch 4: 93.17166900634766\n",
            "\n",
            "iteration 5 is starting...\n",
            "\n",
            "Train Accuracy for Epoch5: 91.53666687011719\n",
            "Test Accuracy for Epoch 5: 93.92666625976562\n",
            "\n",
            "iteration 6 is starting...\n",
            "\n",
            "Train Accuracy for Epoch6: 92.32833099365234\n",
            "Test Accuracy for Epoch 6: 94.48333740234375\n",
            "\n",
            "iteration 7 is starting...\n",
            "\n",
            "Train Accuracy for Epoch7: 93.08833312988281\n",
            "Test Accuracy for Epoch 7: 94.92166900634766\n",
            "\n",
            "iteration 8 is starting...\n",
            "\n",
            "Train Accuracy for Epoch8: 93.58167266845703\n",
            "Test Accuracy for Epoch 8: 95.23500061035156\n",
            "\n",
            "iteration 9 is starting...\n",
            "\n",
            "Train Accuracy for Epoch9: 93.97333526611328\n",
            "Test Accuracy for Epoch 9: 95.5133285522461\n",
            "\n",
            "iteration 10 is starting...\n",
            "\n",
            "Train Accuracy for Epoch10: 94.32666778564453\n",
            "Test Accuracy for Epoch 10: 95.74166870117188\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#Saving the model\n",
        "torch.save(cnn.state_dict(), 'CNN-MNIST.pth')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_5_X3M4QB7Wt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deAhRiW2bgqV",
        "outputId": "219fae84-222e-436f-aa30-1e23347c596a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('cnn1.weight',\n",
              "              tensor([[[[ 0.1494,  0.1822, -0.0501],\n",
              "                        [-0.0235,  0.1878,  0.0862],\n",
              "                        [ 0.2558, -0.1348, -0.1823]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.3021,  0.0761, -0.3139],\n",
              "                        [ 0.1877, -0.2212,  0.0026],\n",
              "                        [ 0.2406,  0.0936, -0.1457]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0865,  0.3402, -0.0071],\n",
              "                        [ 0.1489,  0.0267,  0.2147],\n",
              "                        [-0.2242, -0.1442,  0.2941]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.3012,  0.1860,  0.1121],\n",
              "                        [-0.1966, -0.2535,  0.1511],\n",
              "                        [-0.2035,  0.0466,  0.1371]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1231, -0.2719,  0.2541],\n",
              "                        [ 0.0071, -0.2421, -0.1276],\n",
              "                        [ 0.2862, -0.0761, -0.1364]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 0.1348,  0.1799,  0.1595],\n",
              "                        [ 0.0257,  0.0016,  0.2014],\n",
              "                        [-0.2562, -0.2740, -0.2512]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.1259,  0.0268,  0.0224],\n",
              "                        [-0.2752, -0.1312,  0.0626],\n",
              "                        [ 0.0964,  0.2707,  0.1677]]],\n",
              "              \n",
              "              \n",
              "                      [[[-0.0863,  0.3375,  0.2896],\n",
              "                        [ 0.0639, -0.0765,  0.2085],\n",
              "                        [-0.3071,  0.0183,  0.1232]]]], device='cuda:0')),\n",
              "             ('cnn1.bias',\n",
              "              tensor([-0.1719,  0.2515, -0.0920,  0.3162, -0.1708,  0.2371, -0.2737, -0.2691],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm1.weight',\n",
              "              tensor([0.9769, 0.9912, 0.9992, 0.9637, 1.0054, 1.0585, 1.0004, 1.0015],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm1.bias',\n",
              "              tensor([-0.0092, -0.0106, -0.0109, -0.0276, -0.0041, -0.0068, -0.0101, -0.0032],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm1.running_mean',\n",
              "              tensor([-0.1632,  0.2399, -0.0819,  0.3305, -0.1713,  0.2324, -0.2672, -0.2567],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm1.running_var',\n",
              "              tensor([0.2613, 0.2227, 0.3213, 0.1667, 0.2140, 0.2746, 0.1731, 0.4342],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm1.num_batches_tracked', tensor(6000, device='cuda:0')),\n",
              "             ('cnn2.weight',\n",
              "              tensor([[[[ 4.7925e-02,  1.1083e-01,  4.6092e-02],\n",
              "                        [-9.0861e-02,  7.8387e-02, -9.1696e-02],\n",
              "                        [ 3.9461e-02, -4.7463e-02, -7.8274e-02]],\n",
              "              \n",
              "                       [[ 8.7571e-02,  4.6805e-02, -9.5127e-02],\n",
              "                        [ 2.2446e-02, -6.5593e-02,  2.0527e-02],\n",
              "                        [-9.7477e-02, -9.7395e-02,  1.3021e-01]],\n",
              "              \n",
              "                       [[-9.8423e-02,  5.6480e-02,  3.8008e-03],\n",
              "                        [-5.4907e-02, -4.2502e-02,  1.1685e-02],\n",
              "                        [ 5.2506e-02,  1.7825e-02,  8.3933e-03]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 6.8725e-02,  7.0285e-02,  1.1278e-01],\n",
              "                        [-5.1395e-02,  9.2283e-02,  9.9092e-02],\n",
              "                        [-1.1625e-02,  1.7140e-02,  6.8534e-03]],\n",
              "              \n",
              "                       [[-5.7278e-02,  1.6784e-02, -1.1059e-01],\n",
              "                        [-4.7986e-02,  8.9088e-02, -1.1239e-01],\n",
              "                        [-6.0990e-02,  1.1370e-01, -2.7104e-02]],\n",
              "              \n",
              "                       [[-5.7584e-02, -8.1359e-02,  1.1462e-01],\n",
              "                        [-6.1496e-02, -9.7793e-02,  7.7631e-02],\n",
              "                        [-3.0053e-02,  2.5428e-02,  8.2006e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[-1.7217e-03,  1.8185e-02, -2.2266e-02],\n",
              "                        [-1.0397e-01, -1.0950e-01,  8.9227e-02],\n",
              "                        [-2.0608e-02, -6.7915e-02, -4.4740e-02]],\n",
              "              \n",
              "                       [[-1.0000e-01, -6.1283e-02,  4.2013e-02],\n",
              "                        [-5.3861e-02,  1.9214e-02, -6.5003e-02],\n",
              "                        [-3.5418e-02,  4.0974e-02,  3.2154e-02]],\n",
              "              \n",
              "                       [[ 7.6933e-02,  6.4159e-02,  5.2033e-02],\n",
              "                        [ 7.6335e-02, -3.9724e-02, -9.4007e-02],\n",
              "                        [ 1.4294e-02,  7.2783e-02, -4.9576e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 1.4960e-01, -3.8528e-03,  1.2633e-03],\n",
              "                        [-7.3238e-02,  2.2458e-02,  5.6073e-02],\n",
              "                        [ 3.5192e-02,  1.0248e-01,  3.7823e-02]],\n",
              "              \n",
              "                       [[ 6.1987e-03, -3.5136e-02,  5.3595e-02],\n",
              "                        [-1.2812e-01, -1.0430e-01,  5.7307e-02],\n",
              "                        [ 4.2783e-02, -1.5770e-02,  1.9396e-03]],\n",
              "              \n",
              "                       [[ 1.2003e-01,  6.9543e-02,  7.5483e-02],\n",
              "                        [-4.2257e-02, -7.3235e-02,  6.5544e-02],\n",
              "                        [-5.8443e-02, -8.3462e-02, -5.3940e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 8.5822e-02,  8.2298e-02, -8.2590e-02],\n",
              "                        [-2.8401e-02,  8.9142e-02,  9.5836e-02],\n",
              "                        [ 2.6133e-02,  6.5364e-02,  1.7366e-02]],\n",
              "              \n",
              "                       [[-9.8851e-02, -8.1257e-04, -8.6874e-02],\n",
              "                        [-5.6970e-02,  1.6330e-02, -3.8260e-02],\n",
              "                        [-8.6078e-02,  3.6752e-02,  5.7108e-02]],\n",
              "              \n",
              "                       [[-7.6541e-02, -3.6507e-03,  1.0788e-02],\n",
              "                        [-2.6996e-02, -3.0356e-02,  2.5654e-02],\n",
              "                        [-1.1110e-03, -2.5066e-02,  6.8649e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-9.7644e-02, -4.2254e-03, -4.4893e-02],\n",
              "                        [-5.0348e-02,  7.7627e-02,  1.4234e-02],\n",
              "                        [-1.1616e-01,  5.0169e-02,  9.7645e-02]],\n",
              "              \n",
              "                       [[ 1.0879e-01, -5.3493e-02,  7.6894e-02],\n",
              "                        [ 4.3511e-02, -1.2320e-01,  8.7909e-02],\n",
              "                        [-8.6499e-02, -1.1181e-01, -6.3527e-02]],\n",
              "              \n",
              "                       [[ 7.9679e-02,  7.0397e-02,  5.5018e-02],\n",
              "                        [ 6.5306e-02,  2.6996e-02, -1.1268e-01],\n",
              "                        [ 6.2497e-02, -4.0657e-02, -2.9424e-04]]],\n",
              "              \n",
              "              \n",
              "                      ...,\n",
              "              \n",
              "              \n",
              "                      [[[-1.0423e-01,  4.2028e-02, -7.8292e-02],\n",
              "                        [-3.5642e-02,  1.5071e-02, -7.8020e-03],\n",
              "                        [-2.2273e-02, -4.3750e-02, -4.5409e-02]],\n",
              "              \n",
              "                       [[ 1.0499e-02,  3.5412e-02,  2.2991e-03],\n",
              "                        [ 3.8189e-02,  4.2220e-02,  4.1721e-02],\n",
              "                        [-2.4217e-02,  2.7668e-03, -8.8753e-02]],\n",
              "              \n",
              "                       [[ 4.6351e-02, -8.5957e-02,  2.0336e-02],\n",
              "                        [ 1.2409e-01,  3.8526e-02,  1.3283e-02],\n",
              "                        [ 1.4611e-02,  1.0443e-01,  1.1571e-01]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[-2.3455e-02,  2.0035e-03,  2.9472e-02],\n",
              "                        [ 5.6900e-02, -1.1244e-01, -4.7607e-02],\n",
              "                        [-5.3866e-03,  9.4763e-02, -1.0118e-01]],\n",
              "              \n",
              "                       [[ 5.3211e-02,  3.1453e-02, -8.1541e-02],\n",
              "                        [-8.7400e-02, -1.0222e-01,  8.1154e-02],\n",
              "                        [-1.0558e-01, -6.8703e-02,  2.7282e-02]],\n",
              "              \n",
              "                       [[ 4.4805e-02,  5.0300e-02,  7.9288e-03],\n",
              "                        [-2.9565e-02, -5.9615e-02,  7.3427e-02],\n",
              "                        [ 1.2796e-01,  5.5815e-02,  1.0440e-01]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 6.8786e-02,  7.5584e-02, -1.1103e-01],\n",
              "                        [ 7.2437e-02,  1.1740e-02, -3.8049e-02],\n",
              "                        [-1.1927e-01, -6.4638e-02, -7.6147e-02]],\n",
              "              \n",
              "                       [[-1.2301e-01, -1.1031e-01, -1.0744e-01],\n",
              "                        [-1.1218e-01, -6.7886e-02,  9.1766e-02],\n",
              "                        [ 1.3141e-03,  1.1170e-01,  1.1473e-01]],\n",
              "              \n",
              "                       [[-4.7739e-02, -7.9070e-02, -6.8848e-02],\n",
              "                        [ 5.5278e-02, -5.4073e-02,  5.8968e-02],\n",
              "                        [-4.1306e-02, -1.0903e-01, -7.6805e-02]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 1.0304e-02, -3.7701e-02, -1.0385e-01],\n",
              "                        [-8.5265e-02, -1.1639e-01, -4.3086e-02],\n",
              "                        [ 4.4646e-02, -3.5715e-02,  7.8032e-02]],\n",
              "              \n",
              "                       [[ 8.5038e-02,  2.7537e-02, -7.9987e-03],\n",
              "                        [-1.9130e-02, -3.6407e-02,  6.8136e-02],\n",
              "                        [-8.1587e-02,  1.2623e-01,  8.6482e-03]],\n",
              "              \n",
              "                       [[-4.2472e-02,  9.5935e-02, -7.5845e-02],\n",
              "                        [ 2.2028e-02,  8.4484e-02, -5.4226e-02],\n",
              "                        [-9.2733e-02, -5.8563e-02, -6.4985e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 9.2161e-02,  7.2614e-02, -4.3440e-02],\n",
              "                        [ 2.3936e-02, -6.2947e-02, -1.0598e-01],\n",
              "                        [ 5.8291e-02, -3.2821e-02,  1.1982e-01]],\n",
              "              \n",
              "                       [[ 9.2477e-03, -8.6946e-02, -3.0238e-02],\n",
              "                        [ 9.4692e-03,  3.2142e-02,  1.3872e-02],\n",
              "                        [ 1.1601e-01,  1.0633e-01, -8.4986e-03]],\n",
              "              \n",
              "                       [[-1.0639e-01, -1.9666e-02,  8.2594e-02],\n",
              "                        [ 7.6672e-02,  6.7232e-03, -5.1775e-02],\n",
              "                        [ 8.8675e-02, -1.6655e-02,  2.5803e-05]],\n",
              "              \n",
              "                       ...,\n",
              "              \n",
              "                       [[ 5.6727e-02,  7.5343e-02, -3.1637e-02],\n",
              "                        [-7.2500e-02, -5.9508e-02, -1.3467e-02],\n",
              "                        [ 8.8294e-02, -5.0699e-02,  1.1603e-01]],\n",
              "              \n",
              "                       [[-9.4940e-02,  1.8654e-02,  3.1833e-02],\n",
              "                        [ 2.9973e-02,  1.4337e-01,  7.3431e-02],\n",
              "                        [ 2.7975e-02, -3.7352e-02, -2.9493e-02]],\n",
              "              \n",
              "                       [[-1.2134e-01,  2.7040e-02,  9.2906e-02],\n",
              "                        [-1.2980e-02, -3.8840e-02,  6.6320e-02],\n",
              "                        [-3.7043e-02,  1.2345e-01,  6.4942e-02]]]], device='cuda:0')),\n",
              "             ('cnn2.bias',\n",
              "              tensor([-0.0804,  0.0264,  0.0120, -0.1041, -0.0151, -0.0027,  0.0012, -0.0151,\n",
              "                       0.0125,  0.1042, -0.0917,  0.0678,  0.0177, -0.0884, -0.0850, -0.0915,\n",
              "                      -0.0868, -0.0118,  0.0232, -0.0243,  0.0192, -0.0197, -0.0724, -0.0038,\n",
              "                       0.0471,  0.0667,  0.0687, -0.0791,  0.0361,  0.0087, -0.0046,  0.0036],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm2.weight',\n",
              "              tensor([1.0287, 1.0440, 1.0482, 1.0341, 1.0781, 1.0616, 1.0563, 1.0609, 1.0390,\n",
              "                      1.0387, 1.0589, 1.0641, 1.0807, 1.0347, 1.0265, 1.0395, 1.0290, 1.0507,\n",
              "                      1.0445, 1.0197, 1.0612, 1.0657, 1.0477, 1.0614, 1.0428, 1.0497, 1.0535,\n",
              "                      1.0798, 1.0386, 1.0515, 1.0116, 1.0313], device='cuda:0')),\n",
              "             ('batchnorm2.bias',\n",
              "              tensor([0.0149, 0.0074, 0.0114, 0.0118, 0.0105, 0.0092, 0.0144, 0.0120, 0.0144,\n",
              "                      0.0112, 0.0158, 0.0051, 0.0102, 0.0165, 0.0071, 0.0061, 0.0064, 0.0090,\n",
              "                      0.0123, 0.0105, 0.0059, 0.0101, 0.0138, 0.0143, 0.0083, 0.0064, 0.0129,\n",
              "                      0.0122, 0.0065, 0.0095, 0.0153, 0.0046], device='cuda:0')),\n",
              "             ('batchnorm2.running_mean',\n",
              "              tensor([-0.1760, -0.0863, -0.0474, -0.0034,  0.2813,  0.2759,  0.2974,  0.1314,\n",
              "                      -0.0619,  0.1071,  0.2391,  0.0719, -0.0926, -0.0682,  0.1465,  0.1911,\n",
              "                       0.2356,  0.1243,  0.0885, -0.0259,  0.1082, -0.1595,  0.0209,  0.1690,\n",
              "                       0.1573,  0.1863,  0.1598, -0.0837,  0.3024,  0.1395, -0.3173,  0.4380],\n",
              "                     device='cuda:0')),\n",
              "             ('batchnorm2.running_var',\n",
              "              tensor([0.1584, 0.2604, 0.1580, 0.4117, 0.1788, 0.1748, 0.1941, 0.3562, 0.0940,\n",
              "                      0.4616, 0.5223, 0.1353, 0.3066, 0.1521, 0.2025, 0.2046, 0.3945, 0.1368,\n",
              "                      0.2402, 0.2642, 0.3842, 0.1163, 0.1642, 0.1125, 0.2754, 0.0912, 0.3460,\n",
              "                      0.1365, 0.2958, 0.1726, 0.4191, 0.4576], device='cuda:0')),\n",
              "             ('batchnorm2.num_batches_tracked', tensor(6000, device='cuda:0')),\n",
              "             ('fc1.weight',\n",
              "              tensor([[ 0.0007,  0.0049,  0.0160,  ..., -0.0158,  0.0241, -0.0053],\n",
              "                      [ 0.0121,  0.0148, -0.0052,  ..., -0.0214, -0.0124,  0.0139],\n",
              "                      [ 0.0077, -0.0205, -0.0037,  ..., -0.0092,  0.0235, -0.0082],\n",
              "                      ...,\n",
              "                      [ 0.0048,  0.0017, -0.0072,  ..., -0.0159, -0.0108,  0.0054],\n",
              "                      [ 0.0235, -0.0235,  0.0077,  ..., -0.0139, -0.0035,  0.0219],\n",
              "                      [ 0.0168,  0.0025, -0.0197,  ..., -0.0222, -0.0037,  0.0234]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc1.bias',\n",
              "              tensor([-2.1069e-02, -2.0463e-02,  1.5377e-02,  1.9120e-02,  1.0013e-02,\n",
              "                       8.4560e-05,  8.8530e-03, -7.0433e-03,  1.8515e-02, -1.4675e-02,\n",
              "                      -2.1216e-02, -8.8996e-04,  1.4554e-02,  9.4405e-03,  8.4191e-03,\n",
              "                       2.0726e-02, -1.6438e-02,  1.9756e-02, -1.6589e-02, -2.2892e-02,\n",
              "                      -7.1657e-03,  1.5867e-03,  1.4988e-02, -1.9466e-02,  1.2865e-02,\n",
              "                      -8.7536e-03, -2.3957e-02,  2.0457e-02, -1.8173e-02,  2.6699e-03,\n",
              "                      -6.3369e-03, -1.1567e-03,  2.3808e-02,  5.0766e-03,  2.7883e-02,\n",
              "                      -5.8102e-03,  1.1350e-02, -1.1667e-02,  2.4498e-03,  1.0454e-02,\n",
              "                      -1.6393e-02,  2.1582e-02,  1.6095e-02, -1.3404e-02,  4.1881e-03,\n",
              "                      -2.5280e-02, -6.4240e-03,  1.0919e-02, -1.6249e-02,  1.6195e-02,\n",
              "                      -1.1771e-02,  1.3131e-02, -5.1423e-04, -9.2973e-03,  8.4856e-03,\n",
              "                      -2.3471e-02, -1.5715e-02,  6.9184e-03,  5.8196e-03,  7.1591e-03,\n",
              "                       2.0352e-04,  1.4086e-03,  1.1652e-02, -1.8514e-02,  2.1346e-02,\n",
              "                       2.3678e-03,  1.2708e-02, -1.9420e-02, -3.6952e-03, -3.6105e-03,\n",
              "                      -2.4797e-02, -4.9419e-03, -8.9522e-03,  1.8460e-02,  1.4939e-02,\n",
              "                      -1.9809e-02,  1.2434e-02, -1.6572e-02,  1.7555e-02, -2.1005e-02,\n",
              "                      -5.1297e-03,  2.0478e-02,  3.5636e-03, -1.6492e-03, -8.7162e-03,\n",
              "                      -1.6239e-02,  2.5521e-02, -5.5688e-03, -1.4457e-03, -2.2991e-02,\n",
              "                       9.2064e-03, -3.2342e-03,  1.2495e-02, -1.2849e-02,  2.1650e-02,\n",
              "                       3.8970e-03,  2.5378e-02, -2.2277e-02, -7.0278e-03,  1.9932e-02,\n",
              "                       2.1449e-02,  1.1457e-02, -8.4783e-03, -1.6167e-02, -1.4995e-02,\n",
              "                       1.3899e-02,  1.5235e-02, -2.1962e-02,  3.3044e-03,  1.3687e-02,\n",
              "                      -9.9106e-03,  1.9782e-02,  3.9646e-03,  3.2880e-03, -1.4060e-02,\n",
              "                       1.3461e-03,  2.8086e-03,  8.1552e-03, -1.1987e-02, -8.3353e-03,\n",
              "                      -2.5681e-02, -2.4243e-02,  4.6156e-04,  1.8285e-02, -1.2021e-02,\n",
              "                       1.2581e-02,  1.5180e-02,  2.1669e-02,  2.2336e-04, -3.5653e-03,\n",
              "                      -1.3396e-03, -2.1801e-02,  1.6573e-02,  1.6616e-02, -3.7250e-03,\n",
              "                       3.9651e-04,  2.1197e-02, -1.6391e-03,  1.6642e-02, -7.4063e-03,\n",
              "                       3.2247e-03,  1.1680e-02,  1.9317e-02,  1.5429e-02,  2.2367e-02,\n",
              "                      -5.5849e-03,  1.6301e-02, -1.2064e-02,  1.1069e-02,  1.0628e-02,\n",
              "                       2.2566e-02,  8.2173e-03, -1.9503e-02,  1.3113e-02,  1.1624e-02,\n",
              "                      -2.2752e-02, -2.1959e-02,  6.2475e-03,  1.4030e-03,  1.7545e-02,\n",
              "                       2.4385e-02, -1.0803e-02,  2.6068e-02, -2.3928e-02,  1.3473e-03,\n",
              "                       1.7998e-03,  2.2084e-02,  1.9873e-02,  1.0894e-02, -1.1165e-02,\n",
              "                       2.2995e-02,  6.8239e-04, -1.7423e-03,  2.4548e-02,  2.3906e-02,\n",
              "                      -6.4390e-03,  2.2069e-02,  2.2881e-02, -4.8957e-03,  1.7480e-03,\n",
              "                      -1.8615e-02,  2.0930e-04, -1.0892e-02,  1.6922e-02,  1.5996e-02,\n",
              "                      -1.9040e-03, -2.3215e-02,  2.2062e-02,  2.3225e-02, -1.4066e-02,\n",
              "                      -1.0509e-02, -1.6586e-02,  1.2707e-02,  1.9579e-02,  1.1619e-02,\n",
              "                       1.3218e-02,  1.1858e-02,  1.5085e-02,  1.8570e-03,  2.5821e-02,\n",
              "                       2.2919e-02, -5.0988e-03, -2.0197e-02,  2.0098e-02,  5.0548e-04,\n",
              "                       1.1982e-02, -1.7368e-02, -1.8546e-02, -2.1950e-02,  3.4683e-03,\n",
              "                       6.9827e-03,  5.0823e-03,  4.4952e-03,  1.5089e-02,  9.1514e-03,\n",
              "                      -1.1849e-02, -2.2143e-02,  2.4792e-02, -3.4813e-03, -5.2661e-03,\n",
              "                      -2.1796e-02,  5.2640e-03, -1.6569e-02,  1.0328e-02,  2.2357e-02,\n",
              "                       1.7820e-02,  1.9200e-02, -1.1508e-02,  6.9119e-03, -2.0257e-02,\n",
              "                      -8.2557e-04,  1.8068e-04,  1.0430e-02,  1.1855e-02,  4.3829e-03,\n",
              "                       2.2449e-03,  8.6623e-03, -2.1217e-02, -1.3691e-02,  1.8915e-02,\n",
              "                       1.0932e-02, -1.2396e-02, -3.9009e-03, -1.0603e-02, -2.4122e-02,\n",
              "                      -1.0099e-02, -2.1827e-02, -9.0056e-03,  2.4438e-02,  1.6941e-02,\n",
              "                      -1.5529e-02, -2.0983e-03, -1.3768e-02, -2.4872e-02, -2.0421e-02,\n",
              "                      -4.2688e-03,  1.6410e-02,  1.2681e-02,  8.9347e-03,  1.3339e-02,\n",
              "                      -1.4828e-02, -2.6462e-02,  1.0456e-02,  2.4074e-02, -2.8653e-03,\n",
              "                      -1.7429e-03,  7.5150e-03,  2.5364e-02, -1.7420e-02,  1.6556e-02,\n",
              "                       5.8243e-03,  9.9158e-03, -1.7759e-02, -1.9336e-02, -7.6056e-03,\n",
              "                      -2.4342e-03,  2.4419e-02, -2.5197e-03, -1.8297e-02, -1.7893e-02,\n",
              "                       1.1690e-02, -1.4142e-02,  2.8145e-03, -8.0985e-03,  1.8704e-02,\n",
              "                      -2.0345e-02, -6.6580e-03, -7.3096e-03,  1.0424e-02,  1.0691e-02,\n",
              "                       2.1530e-02,  1.0675e-02,  1.1639e-02,  2.2633e-02,  1.1343e-02,\n",
              "                       1.2733e-02, -1.0043e-03, -4.7614e-04,  7.9937e-03, -1.8079e-02,\n",
              "                       2.3079e-02, -1.5229e-02,  1.2533e-02,  1.3585e-02, -7.6598e-03,\n",
              "                      -1.0334e-02, -8.7370e-03, -2.8398e-03,  2.2685e-02, -6.6992e-03,\n",
              "                      -7.2904e-03, -1.2797e-02,  9.8672e-03, -1.9739e-02,  2.7145e-03,\n",
              "                       2.1179e-02, -1.9538e-02, -2.1373e-03, -5.0687e-03,  2.4903e-02,\n",
              "                       1.0663e-02,  3.5392e-03,  8.3186e-03,  1.0581e-02, -1.3737e-02,\n",
              "                       4.4918e-03, -1.9118e-02,  1.5342e-02, -2.5208e-02,  1.8750e-02,\n",
              "                      -2.3818e-02, -1.3222e-03,  2.2203e-02,  1.8603e-02,  1.1627e-02,\n",
              "                      -8.6522e-03,  6.2000e-04,  1.3213e-02, -1.4005e-02,  3.0968e-03,\n",
              "                      -1.9224e-02, -1.1396e-02,  2.2007e-02,  1.5406e-02, -1.7673e-02,\n",
              "                       1.7570e-02,  2.3858e-02,  8.3161e-03, -7.0853e-03,  2.5611e-02,\n",
              "                      -2.5806e-03, -4.4166e-03,  1.2330e-02,  3.4230e-03, -7.9030e-03,\n",
              "                      -1.2229e-02,  2.4049e-03,  2.2410e-02,  1.4322e-02,  1.7568e-03,\n",
              "                      -1.2449e-02,  2.3274e-02,  2.4767e-03, -1.2534e-02, -1.2067e-02,\n",
              "                      -2.2501e-02,  2.3179e-02, -2.2166e-02,  3.3685e-03, -1.7053e-02,\n",
              "                       2.2327e-02, -1.4981e-02,  2.1830e-02, -2.2808e-02,  9.3155e-03,\n",
              "                      -1.2495e-02,  2.1379e-02,  2.3408e-02, -1.1553e-04,  4.6965e-04,\n",
              "                       1.7127e-02,  4.2349e-03,  2.4318e-02, -8.1261e-03, -3.9575e-03,\n",
              "                      -6.4965e-03,  1.1445e-02, -3.2181e-03, -2.0448e-02,  2.3770e-02,\n",
              "                       9.3993e-03, -1.8892e-02, -5.3640e-03,  2.0126e-02, -2.2802e-02,\n",
              "                      -1.9033e-02, -1.5926e-02, -1.6613e-02,  8.2112e-03,  1.9307e-02,\n",
              "                      -1.4766e-02, -1.2625e-03, -1.1117e-02,  1.5218e-02, -2.1601e-02,\n",
              "                      -3.4649e-03,  4.5070e-03, -2.6164e-02, -2.6210e-02,  1.8164e-03,\n",
              "                      -2.4102e-02,  1.9332e-02, -2.4563e-03, -1.4210e-02,  1.0246e-02,\n",
              "                       3.2732e-03, -1.1536e-02,  3.9525e-03, -2.0548e-04,  1.6666e-02,\n",
              "                      -1.5367e-03,  4.9701e-03,  1.6695e-02,  2.1288e-02,  5.4078e-04,\n",
              "                       1.4444e-02,  2.0994e-03,  6.0536e-04, -1.8295e-02,  2.5726e-02,\n",
              "                       1.9768e-02, -5.3972e-03, -2.4038e-02,  2.0689e-02, -2.3786e-02,\n",
              "                       5.6182e-03, -4.2657e-03, -1.2033e-02, -1.4476e-02,  1.9624e-02,\n",
              "                       2.3500e-02,  2.0093e-02, -2.3148e-02, -1.1831e-02, -1.5933e-02,\n",
              "                      -9.1372e-03,  1.3857e-02, -6.8195e-03, -1.7933e-02,  5.2739e-04,\n",
              "                       2.0682e-02, -1.8358e-02,  1.3927e-02,  1.2759e-02,  2.4462e-02,\n",
              "                       2.2849e-03,  1.2646e-02,  2.0456e-02,  2.2033e-02, -5.8226e-03,\n",
              "                       2.3681e-02, -1.3548e-02, -6.1792e-03,  1.2717e-02,  8.2798e-03,\n",
              "                       2.7979e-03,  1.8945e-02,  1.3182e-02, -2.0340e-02,  7.3203e-03,\n",
              "                      -8.0179e-03, -7.3241e-03,  2.2133e-02,  2.1241e-02, -1.8136e-02,\n",
              "                       1.5637e-02,  1.5287e-02, -6.3288e-05, -1.3725e-04, -2.2176e-02,\n",
              "                       2.1701e-02, -7.5125e-04,  1.4004e-02, -8.0416e-03, -9.0607e-03,\n",
              "                       2.2610e-02, -3.1443e-03,  7.5590e-03,  1.1514e-02, -5.6464e-03,\n",
              "                       1.5848e-02, -2.1624e-02,  2.1337e-02, -5.1270e-03,  2.3784e-02,\n",
              "                       2.6954e-02, -2.1458e-03,  1.2995e-02,  1.5830e-02, -1.2738e-02,\n",
              "                      -1.1310e-02,  1.3645e-02,  2.3389e-02, -1.9784e-02,  1.5161e-02,\n",
              "                      -2.6790e-03,  1.0690e-02, -4.3683e-03, -6.3742e-03, -2.2668e-02,\n",
              "                      -1.0011e-02,  2.9001e-03, -1.8897e-02,  2.4534e-02,  3.0849e-03,\n",
              "                      -4.0732e-03,  8.6726e-03, -1.8308e-02,  8.8975e-03, -1.2175e-02,\n",
              "                       1.3776e-02, -2.4297e-02, -5.7595e-03,  1.7188e-02,  1.8715e-02,\n",
              "                       1.8942e-03, -4.0852e-03,  2.5734e-02, -1.2253e-02,  1.8970e-02,\n",
              "                       2.7631e-03, -8.9121e-03,  2.5219e-02,  1.3731e-02,  2.5148e-02,\n",
              "                       1.6768e-02,  9.7099e-03,  2.1287e-02,  2.2217e-02, -6.5577e-04,\n",
              "                      -1.8873e-02,  3.6527e-03,  1.6896e-02,  1.4625e-02, -1.9407e-02,\n",
              "                       1.8003e-02, -2.2862e-02,  1.9933e-02,  1.0861e-02,  2.4494e-02,\n",
              "                      -1.4639e-02, -2.2923e-02,  1.3984e-02,  1.8140e-02,  7.6755e-03,\n",
              "                      -3.8653e-03, -1.5888e-02,  2.5117e-02,  2.1423e-02,  1.8656e-02,\n",
              "                       2.2901e-02,  2.6768e-02,  1.4707e-02, -1.3082e-02,  2.7796e-04,\n",
              "                      -1.8938e-02, -4.3242e-03,  1.5035e-02,  2.9031e-03, -2.1882e-02,\n",
              "                      -1.6310e-02, -2.2501e-02,  1.3712e-02,  2.4865e-02, -1.7967e-02,\n",
              "                      -2.3781e-02, -2.2947e-02, -1.0907e-02,  2.2122e-02, -9.1626e-04,\n",
              "                      -1.1701e-02,  2.7204e-02, -1.5253e-02,  1.1368e-03,  1.0554e-02,\n",
              "                       1.8590e-02, -1.0538e-02, -1.5187e-02, -6.6291e-03,  2.1705e-02,\n",
              "                       1.1275e-02,  1.0648e-02,  5.3714e-03, -2.4341e-02,  1.0343e-02,\n",
              "                      -2.3147e-02,  5.3740e-03, -5.6199e-03,  2.6363e-02,  1.8338e-02],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-0.0554,  0.0001, -0.0062,  ..., -0.0307,  0.0283,  0.0602],\n",
              "                      [-0.0326, -0.0066, -0.0379,  ...,  0.0386,  0.0573, -0.0280],\n",
              "                      [ 0.0244, -0.0383,  0.0679,  ...,  0.0181, -0.0384,  0.0051],\n",
              "                      ...,\n",
              "                      [-0.0006, -0.0285, -0.0170,  ..., -0.0215, -0.0161,  0.0578],\n",
              "                      [-0.0421, -0.0317, -0.0360,  ...,  0.0266, -0.0470,  0.0307],\n",
              "                      [ 0.0007, -0.0036, -0.0341,  ..., -0.0299, -0.0272,  0.0043]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.bias',\n",
              "              tensor([ 0.0034, -0.0108,  0.0251,  0.0207,  0.0457,  0.0356, -0.0144, -0.0312,\n",
              "                       0.0154, -0.0032], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}